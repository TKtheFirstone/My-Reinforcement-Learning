# -*- coding: utf-8 -*-
"""My Reinforcement Learning

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rtYYZeLwpmAkUcFix88LiIetm-6RIGvd

# **Situation** -

```
There are two doors for our agent to choose from.
In first one, there is +10 reward for opening it always.
In second door, there is 20% probability of +200 reward for opening it, and 80% probability of getting -10 penalty.
```
"""

# installing dependencies
!pip install gym
# !pip install gymnasium - latest version

"""**Key Terms & Concepts:**

* State: In RL, a state represents the current situation of the agent. In this problem, we have a single state since the agent always has two doors to choose from.  
* Action: Choices the agent can make. Here, the actions are choosing door 1 or door 2.  
* Reward: The feedback received after taking an action in a state. Here, rewards are given based on opening the doors.  
* Q-values: Q-values denote the expected future reward of an action taken in a state. Will use a Q-table to store these values.  

* **Exploration vs Exploitation**: Initially, the agent will explore actions randomly (exploration). As it learns, it will start choosing the best actions it knows (exploitation).

**Steps:**

Initialization: Initialize Q-values to zeros.
Policy: Define a policy to choose actions. We'll use an epsilon-greedy policy.  
Learning: Update Q-values using the Q-learning update rule.  
Iteration: Repeat the policy and learning steps.
"""

import gym
import numpy as np

# Environment
num_doors = 2
reward_door1 = 10
reward_door2 = [200, -10]
prob_door2 = [0.2, 0.8]

# Q-learning parameters
alpha = 0.1
gamma = 0.9
epsilon = 0.1
episodes = 1000

# Q-table initialization
q_table = np.zeros(num_doors)

class Choice:
  def run(self):
    for _ in range(episodes):
      # Epsilon-greedy policy
      if np.random.uniform(0, 1) < epsilon:
          action = np.random.choice(num_doors)
      else:
          action = np.argmax(q_table)

      # Get the reward
      if action == 0:
          reward = reward_door1
      else:
          reward = np.random.choice(reward_door2, p=prob_door2)

      # Q-learning update rule
      q_table[action] = q_table[action] + alpha * (reward + gamma * np.max(q_table) - q_table[action])

    print("Q-values:", q_table)
    best_door = np.argmax(q_table) + 1
    print(f"Best door to choose is Door {best_door}")

RL = Choice()
RL.run()