{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNLSUOZJq7BtcSD2C/F/4oR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TKtheFirstone/My-Reinforcement-Learning/blob/main/My_Reinforcement_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Aim -\n",
        "```\n",
        "There are two doors for our agent to choose from.\n",
        "In first one, there is +10 reward for opening it always.\n",
        "In second door, there is 20% probability of +200 reward for opening it, and 80% probability of getting -10 penalty.\n",
        "```\n"
      ],
      "metadata": {
        "id": "ku6UkTs0sNvn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# installing dependencies\n",
        "!pip install gym\n",
        "# !pip install gymnasium - latest version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i69jDU3RpQ7Y",
        "outputId": "fe75a774-0f28-40cd-dd75-b85b3fdb3119"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.10/dist-packages (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym) (1.23.5)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym) (0.0.8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Key Terms & Concepts:**\n",
        "\n",
        "* State: In RL, a state represents the current situation of the agent. In this problem, we have a single state since the agent always has two doors to choose from.  \n",
        "* Action: Choices the agent can make. Here, the actions are choosing door 1 or door 2.  \n",
        "* Reward: The feedback received after taking an action in a state. Here, rewards are given based on opening the doors.  \n",
        "* Q-values: Q-values denote the expected future reward of an action taken in a state. Will use a Q-table to store these values.  \n",
        "\n",
        "* **Exploration vs Exploitation**: Initially, the agent will explore actions randomly (exploration). As it learns, it will start choosing the best actions it knows (exploitation).\n"
      ],
      "metadata": {
        "id": "nk1dHgWbs061"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Steps:**\n",
        "\n",
        "Initialization: Initialize Q-values to zeros.\n",
        "Policy: Define a policy to choose actions. We'll use an epsilon-greedy policy.  \n",
        "Learning: Update Q-values using the Q-learning update rule.  \n",
        "Iteration: Repeat the policy and learning steps.  "
      ],
      "metadata": {
        "id": "SC1PhcBhrfkR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "# Environment\n",
        "num_doors = 2\n",
        "reward_door1 = 10\n",
        "reward_door2 = [200, -10]\n",
        "prob_door2 = [0.2, 0.8]\n",
        "\n",
        "# Q-learning parameters\n",
        "alpha = 0.1\n",
        "gamma = 0.9\n",
        "epsilon = 0.1\n",
        "episodes = 1000\n",
        "\n",
        "# Q-table initialization\n",
        "q_table = np.zeros(num_doors)"
      ],
      "metadata": {
        "id": "T5q9fcsitGlD"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Choice:\n",
        "  def run(self):\n",
        "    for _ in range(episodes):\n",
        "      # Epsilon-greedy policy\n",
        "      if np.random.uniform(0, 1) < epsilon:\n",
        "          action = np.random.choice(num_doors)\n",
        "      else:\n",
        "          action = np.argmax(q_table)\n",
        "\n",
        "      # Get the reward\n",
        "      if action == 0:\n",
        "          reward = reward_door1\n",
        "      else:\n",
        "          reward = np.random.choice(reward_door2, p=prob_door2)\n",
        "\n",
        "      # Q-learning update rule\n",
        "      q_table[action] = q_table[action] + alpha * (reward + gamma * np.max(q_table) - q_table[action])\n",
        "\n",
        "    print(\"Q-values:\", q_table)\n",
        "    best_door = np.argmax(q_table) + 1\n",
        "    print(f\"Best door to choose is Door {best_door}\")"
      ],
      "metadata": {
        "id": "_ePF5qI-tHYX"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "RL = Choice()\n",
        "RL.run()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5CRwoK2KvDvW",
        "outputId": "3d2aac83-26d4-4e84-c189-695b5dc8646c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q-values: [291.80244844 291.83601929]\n",
            "Best door to choose is Door 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Another sample program to train a mountain car how to climb a hill"
      ],
      "metadata": {
        "id": "bw3AWipiwtZY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "U3k0kXTGomjE",
        "outputId": "d53d5027-70cf-4c2f-aa29-f3057f1c9c83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:256: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.\u001b[0m\n",
            "  deprecation(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----- using Q Learning -----\n",
            "Iteration #1 -- Total reward = -200.\n",
            "Iteration #101 -- Total reward = -200.\n",
            "Iteration #201 -- Total reward = -200.\n",
            "Iteration #301 -- Total reward = -200.\n",
            "Iteration #401 -- Total reward = -200.\n",
            "Iteration #501 -- Total reward = -200.\n",
            "Iteration #601 -- Total reward = -200.\n",
            "Iteration #701 -- Total reward = -200.\n",
            "Iteration #801 -- Total reward = -200.\n",
            "Iteration #901 -- Total reward = -200.\n",
            "Iteration #1001 -- Total reward = -200.\n",
            "Iteration #1101 -- Total reward = -200.\n",
            "Iteration #1201 -- Total reward = -200.\n",
            "Iteration #1301 -- Total reward = -200.\n",
            "Iteration #1401 -- Total reward = -200.\n",
            "Iteration #1501 -- Total reward = -200.\n",
            "Iteration #1601 -- Total reward = -200.\n",
            "Iteration #1701 -- Total reward = -200.\n",
            "Iteration #1801 -- Total reward = -200.\n",
            "Iteration #1901 -- Total reward = -200.\n",
            "Iteration #2001 -- Total reward = -200.\n",
            "Iteration #2101 -- Total reward = -200.\n",
            "Iteration #2201 -- Total reward = -200.\n",
            "Iteration #2301 -- Total reward = -200.\n",
            "Iteration #2401 -- Total reward = -200.\n",
            "Iteration #2501 -- Total reward = -200.\n",
            "Iteration #2601 -- Total reward = -200.\n",
            "Iteration #2701 -- Total reward = -200.\n",
            "Iteration #2801 -- Total reward = -200.\n",
            "Iteration #2901 -- Total reward = -200.\n",
            "Iteration #3001 -- Total reward = -200.\n",
            "Iteration #3101 -- Total reward = -200.\n",
            "Iteration #3201 -- Total reward = -200.\n",
            "Iteration #3301 -- Total reward = -200.\n",
            "Iteration #3401 -- Total reward = -200.\n",
            "Iteration #3501 -- Total reward = -200.\n",
            "Iteration #3601 -- Total reward = -200.\n",
            "Iteration #3701 -- Total reward = -200.\n",
            "Iteration #3801 -- Total reward = -200.\n",
            "Iteration #3901 -- Total reward = -200.\n",
            "Iteration #4001 -- Total reward = -200.\n",
            "Iteration #4101 -- Total reward = -200.\n",
            "Iteration #4201 -- Total reward = -200.\n",
            "Iteration #4301 -- Total reward = -200.\n",
            "Iteration #4401 -- Total reward = -200.\n",
            "Iteration #4501 -- Total reward = -200.\n",
            "Iteration #4601 -- Total reward = -200.\n",
            "Iteration #4701 -- Total reward = -200.\n",
            "Iteration #4801 -- Total reward = -200.\n",
            "Iteration #4901 -- Total reward = -200.\n",
            "Iteration #5001 -- Total reward = -200.\n",
            "Iteration #5101 -- Total reward = -200.\n",
            "Iteration #5201 -- Total reward = -200.\n",
            "Iteration #5301 -- Total reward = -200.\n",
            "Iteration #5401 -- Total reward = -200.\n",
            "Iteration #5501 -- Total reward = -200.\n",
            "Iteration #5601 -- Total reward = -200.\n",
            "Iteration #5701 -- Total reward = -200.\n",
            "Iteration #5801 -- Total reward = -200.\n",
            "Iteration #5901 -- Total reward = -200.\n",
            "Iteration #6001 -- Total reward = -200.\n",
            "Iteration #6101 -- Total reward = -200.\n",
            "Iteration #6201 -- Total reward = -200.\n",
            "Iteration #6301 -- Total reward = -200.\n",
            "Iteration #6401 -- Total reward = -200.\n",
            "Iteration #6501 -- Total reward = -200.\n",
            "Iteration #6601 -- Total reward = -200.\n",
            "Iteration #6701 -- Total reward = -200.\n",
            "Iteration #6801 -- Total reward = -200.\n",
            "Iteration #6901 -- Total reward = -200.\n",
            "Iteration #7001 -- Total reward = -200.\n",
            "Iteration #7101 -- Total reward = -200.\n",
            "Iteration #7201 -- Total reward = -200.\n",
            "Iteration #7301 -- Total reward = -200.\n",
            "Iteration #7401 -- Total reward = -169.\n",
            "Iteration #7501 -- Total reward = -200.\n",
            "Iteration #7601 -- Total reward = -200.\n",
            "Iteration #7701 -- Total reward = -200.\n",
            "Iteration #7801 -- Total reward = -200.\n",
            "Iteration #7901 -- Total reward = -200.\n",
            "Iteration #8001 -- Total reward = -200.\n",
            "Iteration #8101 -- Total reward = -200.\n",
            "Iteration #8201 -- Total reward = -200.\n",
            "Iteration #8301 -- Total reward = -200.\n",
            "Iteration #8401 -- Total reward = -200.\n",
            "Iteration #8501 -- Total reward = -200.\n",
            "Iteration #8601 -- Total reward = -200.\n",
            "Iteration #8701 -- Total reward = -200.\n",
            "Iteration #8801 -- Total reward = -198.\n",
            "Iteration #8901 -- Total reward = -200.\n",
            "Iteration #9001 -- Total reward = -200.\n",
            "Iteration #9101 -- Total reward = -200.\n",
            "Iteration #9201 -- Total reward = -200.\n",
            "Iteration #9301 -- Total reward = -200.\n",
            "Iteration #9401 -- Total reward = -200.\n",
            "Iteration #9501 -- Total reward = -200.\n",
            "Iteration #9601 -- Total reward = -200.\n",
            "Iteration #9701 -- Total reward = -200.\n",
            "Iteration #9801 -- Total reward = -200.\n",
            "Iteration #9901 -- Total reward = -200.\n",
            "Average score of solution =  -142.91\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:49: DeprecationWarning: \u001b[33mWARN: You are calling render method, but you didn't specified the argument render_mode at environment initialization. To maintain backward compatibility, the environment will render in human mode.\n",
            "If you want to render in human mode, initialize the environment in this way: gym.make('EnvName', render_mode='human') and don't call the render method.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ],
      "source": [
        "from gym import wrappers\n",
        "\n",
        "n_states = 40\n",
        "iter_max = 10000\n",
        "\n",
        "initial_lr = 1.0 # Learning rate\n",
        "min_lr = 0.003\n",
        "gamma = 1.0\n",
        "t_max = 10000\n",
        "eps = 0.02\n",
        "\n",
        "def run_episode(env, policy=None, render=False):\n",
        "    obs = env.reset()\n",
        "    total_reward = 0\n",
        "    step_idx = 0\n",
        "    for _ in range(t_max):\n",
        "        if render:\n",
        "            env.render()\n",
        "        if policy is None:\n",
        "            action = env.action_space.sample()\n",
        "        else:\n",
        "            a,b = obs_to_state(env, obs)\n",
        "            action = policy[a][b]\n",
        "        obs, reward, done, _ = env.step(action)\n",
        "        total_reward += gamma ** step_idx * reward\n",
        "        step_idx += 1\n",
        "        if done:\n",
        "            break\n",
        "    return total_reward\n",
        "\n",
        "def obs_to_state(env, obs):\n",
        "    \"\"\" Maps an observation to state \"\"\"\n",
        "    env_low = env.observation_space.low\n",
        "    env_high = env.observation_space.high\n",
        "    env_dx = (env_high - env_low) / n_states\n",
        "    a = int((obs[0] - env_low[0])/env_dx[0])\n",
        "    b = int((obs[1] - env_low[1])/env_dx[1])\n",
        "    return a, b\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    env_name = 'MountainCar-v0'\n",
        "    env = gym.make(env_name)\n",
        "    env.seed(0)\n",
        "    np.random.seed(0)\n",
        "    print ('----- using Q Learning -----')\n",
        "    q_table = np.zeros((n_states, n_states, 3))\n",
        "    for i in range(iter_max):\n",
        "        obs = env.reset()\n",
        "        total_reward = 0\n",
        "        ## eta: learning rate is decreased at each step\n",
        "        eta = max(min_lr, initial_lr * (0.85 ** (i//100)))\n",
        "        for j in range(t_max):\n",
        "            a, b = obs_to_state(env, obs)\n",
        "            if np.random.uniform(0, 1) < eps:\n",
        "                action = np.random.choice(env.action_space.n)\n",
        "            else:\n",
        "                logits = q_table[a][b]\n",
        "                logits_exp = np.exp(logits)\n",
        "                probs = logits_exp / np.sum(logits_exp)\n",
        "                action = np.random.choice(env.action_space.n, p=probs)\n",
        "            obs, reward, done, _ = env.step(action)\n",
        "            total_reward += reward\n",
        "            # update q table\n",
        "            a_, b_ = obs_to_state(env, obs)\n",
        "            q_table[a][b][action] = q_table[a][b][action] + eta * (reward + gamma *  np.max(q_table[a_][b_]) - q_table[a][b][action])\n",
        "            if done:\n",
        "                break\n",
        "        if i % 100 == 0:\n",
        "            print('Iteration #%d -- Total reward = %d.' %(i+1, total_reward))\n",
        "    solution_policy = np.argmax(q_table, axis=2)\n",
        "    solution_policy_scores = [run_episode(env, solution_policy, False) for _ in range(100)]\n",
        "    print(\"Average score of solution = \", np.mean(solution_policy_scores))\n",
        "    # Animate it\n",
        "    run_episode(env, solution_policy, True)"
      ]
    }
  ]
}